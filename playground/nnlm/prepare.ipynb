{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6892d8f3-0ef5-4c27-94cf-76a23406ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.tools import EncodingVisualizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19367313-7459-43e7-b734-4e2cb69a3063",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "https://huggingface.co/datasets/Salesforce/wikitext\n",
    "\n",
    "https://huggingface.co/docs/transformers/perplexity#example-calculating-perplexity-with-gpt-2-in--transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f742c4c-c088-414d-b50e-a1c74480be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/mathewshen/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Mon Feb  3 22:19:08 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a420121-7a63-41b4-8e93-a71da83b12aa",
   "metadata": {},
   "source": [
    "## Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d49c882-e0d2-4388-830e-0163d9ac7f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9127fbb5-e736-4fd4-8a83-7fc291cbb836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(ds, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e32e01-e0c1-4947-a522-b10b91d121d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72e8da3-0d60-4f42-91c7-a83dc14b9acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263e0856-2de9-4357-b10f-61bb75918884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='wikitext', config_name='wikitext-2-raw-v1', version=0.0.0, splits={'test': SplitInfo(name='test', num_bytes=1305088, num_examples=4358, shard_lengths=None, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=11061717, num_examples=36718, shard_lengths=None, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1159288, num_examples=3760, shard_lengths=None, dataset_name='wikitext')}, download_checksums={'hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/test-00000-of-00001.parquet': {'num_bytes': 732610, 'checksum': None}, 'hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/train-00000-of-00001.parquet': {'num_bytes': 6357543, 'checksum': None}, 'hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-2-raw-v1/validation-00000-of-00001.parquet': {'num_bytes': 657209, 'checksum': None}}, download_size=7747362, post_processing_size=None, dataset_size=13526093, size_in_bytes=21273455)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55c70a-769a-4d06-bdc1-eaa3a0594221",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62c4921-5fd5-4d0e-8a85-76bebae76416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['', ' = Valkyria Chronicles III = \\n', '']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9bb8b9-c162-4907-bd67-1933042a14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_word_info(dataset, split):\n",
    "    full_text = \" \".join(dataset[split][\"text\"])\n",
    "    words = full_text.split(\" \")\n",
    "    words_set = set(words)\n",
    "    return len(words), len(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0960863-f824-467b-b96c-5e6376cf8f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2112444, 76618)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split_word_info(ds, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ace7ab9-1bf8-4351-94c0-37166d222e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220111, 19884)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split_word_info(ds, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768eebc3-187d-4a28-a9a0-14d0196aaf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248464, 21143)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split_word_info(ds, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c9457-b7bb-410e-9c3f-95b2cf43981d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aafafb80-63df-45f5-850f-373764db7db1",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "003f6397-f53a-4a46-b1ae-a90d82831dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fb1ae5bbe00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    ds[\"train\"],\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "training_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0953c4ca-7645-48a9-b3be-3b8aab5f099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['', ' = Valkyria Chronicles III = \\n', '', ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\", \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\", '', ' = = Gameplay = = \\n']}\n"
     ]
    }
   ],
   "source": [
    "for batch in training_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "723aeeb6-e755-482b-a96a-a8283eb69fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2403b-f3ba-424c-b6b3-5a451965f426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5df3a15-aff4-44e3-9a2d-713d11dc6dd0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd485e31-6a49-4a84-a96c-51e807e16d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=None, post_processor=None, decoder=None, model=WordLevel(vocab={}, unk_token=\"[UNK]\"))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84972b8-5b15-44a3-9eee-cdfa971115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0fe073f-8021-4dab-bb64-ddd2805ad979",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WordLevelTrainer(vocab_size=20000, min_frequency=3, special_tokens=[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dfadfc0-197a-484c-b5a4-83ce0e826c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(ds[\"train\"][\"text\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff872e73-7f7a-458d-a72d-77a9dfe9e424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token special-token\"  data-stok=\"[UNK]\" >Hello</span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >nice</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >to</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >meet</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >you</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Hello, nice to meet you\"\n",
    "\n",
    "EncodingVisualizer(tokenizer)(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3118ef-e9de-4ea3-92da-672037ae515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', ',', 'nice', 'to', 'meet', 'you']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(text)\n",
    "\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d43bfde-3e88-4722-bc3c-60a8ced232fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 6620, 7, 1609, 324]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4582c8fd-aa31-4b87-af98-22496a56818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e751f659-ac86-4ffe-9faf-b634cd50b5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25aa269d-d2b0-47a5-a02a-4f93c75fcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save('tokenizer-wikitext-2-raw-v1-train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da654d-55fb-488d-95ec-efd9eebb19b9",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bec34be-054c-408d-a906-d36428ec11de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 2, 181, 367]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: Tokenizer) -> list[int]:\n",
    "    return {\"ids\": tokenizer.encode(text).ids}\n",
    "\n",
    "\n",
    "text_to_token_ids(text=\"Hello, World!\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f09342b-83b5-4e0c-a60a-4a8394dffdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10929707"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_split_text_concat(dataset: Dataset, split: str) -> str:\n",
    "    full_text = \" \".join(dataset[split][\"text\"])\n",
    "    return full_text\n",
    "\n",
    "\n",
    "len(dataset_split_text_concat(dataset=ds, split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54230fa2-68f3-4b86-a3b9-1db57d38c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(dataset_split_text_concat(dataset=ds, split=\"train\")).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e2515f0-8daa-4e92-8083-4af90ce651bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074935"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9ceb71e-d491-4b8d-b589-7d2b36893389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    9,  3922,  4421,   852,     9],\n",
       "        [    0,   126,  3922,    89,    43],\n",
       "        [    0,  4421,    23,   755,    43],\n",
       "        ...,\n",
       "        [ 2455,    70, 13989,  2089,     2],\n",
       "        [   29,   146,   299,    35,   398],\n",
       "        [   17,    31,  2493,  5462,     3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_tensor = torch.tensor(token_ids).unfold(0, 5, 5)\n",
    "\n",
    "token_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2609a-8efe-4126-893f-d48087ff9ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toynlp",
   "language": "python",
   "name": "toynlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
