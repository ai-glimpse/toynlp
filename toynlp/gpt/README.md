# GPT



## Pre-Training

### We can overfit a small dataset

```bash
====================================================================================================
[TRAIN] Sample Predictions:
Input tokens: <bos> | # | # | 1 | God | – | Poems | on | God | ,
Target tokens: # | # | 1 | God | – | Poems | on | God | , | Creator
Predicted tokens: # | # | 1 | God | – | Poems | on | God | , | Creator
====================================================================================================
Epoch 758/1000 - Train Loss: 0.0003, Train Perplexity: 1.0003, LR: 0.000100, 
====================================================================================================
```
