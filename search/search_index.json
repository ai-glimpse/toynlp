{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> <p> </p> <p> </p> <p> </p> <p></p>"},{"location":"#toynlp","title":"ToyNLP","text":"<p>Some nlp models with clean implementation and easy-to-use API.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#010-2025-02-16","title":"[0.1.0] - 2025-02-16","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add NNLM model</li> </ul>"},{"location":"models/nnlm/","title":"NNLM","text":""},{"location":"models/nnlm/#toynlp.nnlm.model.NNLM","title":"toynlp.nnlm.model.NNLM","text":"<pre><code>NNLM(config: ModelConfig)\n</code></pre> <p>               Bases: <code>Module</code></p> PARAMETER DESCRIPTION <code>config</code> <p>ModelConfig, the model configuration.</p> <p> TYPE: <code>ModelConfig</code> </p> Source code in <code>toynlp/nnlm/model.py</code> <pre><code>def __init__(\n    self,\n    config: ModelConfig,\n):\n    \"\"\"\n    The Neural Network Language Model (NNLM) model.\n\n    Args:\n        config: ModelConfig, the model configuration.\n    \"\"\"\n    super(NNLM, self).__init__()\n    self.with_direct_connection = config.with_direct_connection\n    self.with_dropout = config.with_dropout\n    # Embedding layer: |V| x m\n    self.C = torch.nn.Embedding(config.vocab_size, config.embedding_dim)\n    self.H = torch.nn.Linear(\n        config.embedding_dim * (config.context_size - 1),\n        config.hidden_dim,\n        bias=False,\n    )\n    self.d = torch.nn.Parameter(torch.zeros(config.hidden_dim))\n    self.U = torch.nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n    self.activation = torch.nn.Tanh()\n\n    self.b = torch.nn.Parameter(torch.zeros(config.vocab_size))\n    self.W = torch.nn.Linear(\n        config.embedding_dim * (config.context_size - 1),\n        config.vocab_size,\n        bias=False,\n    )\n\n    self.dropout = torch.nn.Dropout(config.dropout_rate)\n</code></pre>"},{"location":"models/nnlm/#toynlp.nnlm.model.NNLM.forward","title":"forward","text":"<pre><code>forward(tokens: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass of the model.</p> PARAMETER DESCRIPTION <code>tokens</code> <p>torch.Tensor, (batch_size, seq_len-1), the input tokens.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor, (batch_size, vocab_size), the logits.</p> Source code in <code>toynlp/nnlm/model.py</code> <pre><code>def forward(self, tokens: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        tokens: torch.Tensor, (batch_size, seq_len-1), the input tokens.\n\n    Returns:\n        torch.Tensor, (batch_size, vocab_size), the logits.\n\n    \"\"\"\n    # tokens: (batch_size, seq_len-1) -&gt; x: (batch_size, seq_len-1, embedding_dim)\n    x = self.C(tokens)\n    b, _, _ = x.shape\n    # (batch_size, seq_len-1, embedding_dim) -&gt; (batch_size, embedding_dim * (seq_len-1))\n    x = x.reshape(b, -1)  # (batch_size, embedding_dim * (seq_len-1))\n    if self.with_dropout:\n        x = self.dropout(x)\n    # (batch_size, embedding_dim * (seq_len-1)) -&gt; (batch_size, vocab_size)\n    x1 = self.b + self.U(\n        self.activation(self.H(x) + self.d)\n    )  # no direct connection\n    if not self.with_direct_connection:\n        x = x1\n    else:\n        x2 = self.W(x)\n        x = x1 + x2\n    # return logits\n    return x\n</code></pre>"}]}