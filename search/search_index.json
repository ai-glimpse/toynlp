{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"[![Python](https://img.shields.io/pypi/pyversions/toynlp.svg?color=%2334D058)](https://pypi.org/project/toynlp/) [![PyPI](https://img.shields.io/pypi/v/toynlp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/toynlp/) [![PyPI Downloads](https://static.pepy.tech/badge/toynlp)](https://pepy.tech/projects/toynlp)  [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) [![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)  [![Build Docs](https://github.com/ai-glimpse/toynlp/actions/workflows/build_docs.yaml/badge.svg)](https://github.com/ai-glimpse/toynlp/actions/workflows/build_docs.yaml) [![Test](https://github.com/ai-glimpse/toynlp/actions/workflows/test.yaml/badge.svg)](https://github.com/ai-glimpse/toynlp/actions/workflows/test.yaml) [![Codecov](https://codecov.io/gh/ai-glimpse/toynlp/branch/master/graph/badge.svg)](https://codecov.io/gh/ai-glimpse/toynlp) [![GitHub License](https://img.shields.io/github/license/ai-glimpse/toynlp)](https://github.com/ai-glimpse/toynlp/blob/master/LICENSE)"},{"location":"#toynlp","title":"ToyNLP","text":"<p>NLP models with clean implementation.</p>"},{"location":"#models","title":"Models","text":"<p>10 important NLP models range from 2003 to 2020:</p> <ul> <li> NNLM(2003)</li> <li> Word2Vec(2013)</li> <li> Seq2Seq(2014)</li> <li> Attention(2015)</li> <li> fastText(2016)</li> <li> Transformer(2017)</li> <li> BERT(2018)</li> <li> GPT(2018)</li> <li> XLNet(2019)</li> <li> T5(2020)</li> </ul>"},{"location":"#faq","title":"FAQ","text":""},{"location":"#where-is-gpt-2-and-other-llms","title":"Where is GPT-2 and other LLMs?","text":"<p>Well, it's in toyllm! I separated the models into two libraries, <code>toynlp</code> for traditional \"small\" NLP models and <code>toyllm</code> for LLMs, which are typically larger and more complex.</p>"},{"location":"models/nnlm/","title":"NNLM","text":""},{"location":"models/nnlm/#toynlp.nnlm.model.NNLM","title":"toynlp.nnlm.model.NNLM","text":"<pre><code>NNLM(config: NNLMConfig)\n</code></pre> <p>               Bases: <code>Module</code></p> PARAMETER DESCRIPTION <code>config</code> <p>NNLMConfig, the model configuration.</p> <p> TYPE: <code>NNLMConfig</code> </p> Source code in <code>toynlp/nnlm/model.py</code> <pre><code>def __init__(\n    self,\n    config: NNLMConfig,\n) -&gt; None:\n    \"\"\"The Neural Network Language Model (NNLM) model.\n\n    Args:\n        config: NNLMConfig, the model configuration.\n    \"\"\"\n    super().__init__()\n    self.with_direct_connection = config.with_direct_connection\n    self.with_dropout = config.with_dropout\n    # Embedding layer: |V| x m\n    self.C = torch.nn.Embedding(config.vocab_size, config.embedding_dim)\n    self.H = torch.nn.Linear(\n        config.embedding_dim * (config.context_size - 1),\n        config.hidden_dim,\n        bias=False,\n    )\n    self.d = torch.nn.Parameter(torch.zeros(config.hidden_dim))\n    self.U = torch.nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n    self.activation = torch.nn.Tanh()\n\n    self.b = torch.nn.Parameter(torch.zeros(config.vocab_size))\n    self.W = torch.nn.Linear(\n        config.embedding_dim * (config.context_size - 1),\n        config.vocab_size,\n        bias=False,\n    )\n\n    self.dropout = torch.nn.Dropout(config.dropout_rate)\n</code></pre>"},{"location":"models/nnlm/#toynlp.nnlm.model.NNLM.forward","title":"forward","text":"<pre><code>forward(tokens: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass of the model.</p> PARAMETER DESCRIPTION <code>tokens</code> <p>torch.Tensor, (batch_size, seq_len-1), the input tokens.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor, (batch_size, vocab_size), the logits.</p> Source code in <code>toynlp/nnlm/model.py</code> <pre><code>def forward(self, tokens: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        tokens: torch.Tensor, (batch_size, seq_len-1), the input tokens.\n\n    Returns:\n        torch.Tensor, (batch_size, vocab_size), the logits.\n\n    \"\"\"\n    # tokens: (batch_size, seq_len-1) -&gt; x: (batch_size, seq_len-1, embedding_dim)\n    x = self.C(tokens)\n    b, _, _ = x.shape\n    # (batch_size, seq_len-1, embedding_dim) -&gt; (batch_size, embedding_dim * (seq_len-1))\n    x = x.reshape(b, -1)  # (batch_size, embedding_dim * (seq_len-1))\n    if self.with_dropout:\n        x = self.dropout(x)\n    # (batch_size, embedding_dim * (seq_len-1)) -&gt; (batch_size, vocab_size)\n    x1 = self.b + self.U(\n        self.activation(self.H(x) + self.d),\n    )  # no direct connection\n    if not self.with_direct_connection:\n        x = x1\n    else:\n        x2 = self.W(x)\n        x = x1 + x2\n    # return logits\n    return x\n</code></pre>"}]}